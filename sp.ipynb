{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piero PettenÃ  - RL project\n",
    "Using lecture jupyter notebooks and adapting them to my project. Credit goes to     \n",
    "\n",
    "NOTE: maybe 'transition' and 'rewards' methods can be joint."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 7]\n",
    "plt.rcParams['figure.dpi'] = 100 \n",
    "plt.rcParams['font.size'] = 6\n",
    "\n",
    "def plot_world(World):\n",
    "    # ------------------\n",
    "    Ly, Lx = World.shape\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(World, cmap=plt.get_cmap(\"Spectral\"))\n",
    "    \n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(Lx))\n",
    "    ax.set_yticks(np.arange(Ly))\n",
    "\n",
    "    goal = np.where(np.logical_or( world.grid > 0.0, World < -1.0))\n",
    "    blocks = np.where(World == -1.0)\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(Lx):\n",
    "        for j in range(Ly):\n",
    "            if np.logical_and(goal[0]==j,goal[1]==i).any():\n",
    "                text = ax.text(i,j, 'G{}'.format(int(World[j,i])), ha=\"center\", va=\"center\", color=\"black\")\n",
    "            elif np.logical_and(blocks[0]==j,blocks[1]==i).any():\n",
    "                 text = ax.text(i,j, 'X', ha=\"center\", va=\"center\", color=\"black\", backgroundcolor=\"black\")\n",
    "            else:\n",
    "                pass\n",
    "    plt.show()\n",
    "    # -------------------\n",
    "\n",
    "    \n",
    "\n",
    "def plot_world_values(World, Values):\n",
    "    # ------------------\n",
    "    Ly, Lx = World.shape\n",
    "\n",
    "    fig, (ax, ax2) = plt.subplots(1,2)\n",
    "    im = ax.imshow(World, cmap=plt.get_cmap(\"Spectral\"))\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(Lx))\n",
    "    ax.set_yticks(np.arange(Ly))\n",
    "\n",
    "    goal = np.where(np.logical_or( World > 0.0, World < -1.0))\n",
    "    blocks = np.where(World == -1.0)\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(Lx):\n",
    "        for j in range(Ly):\n",
    "            if np.logical_and(goal[0]==j,goal[1]==i).any():\n",
    "                text = ax.text(i,j, 'G{}'.format(World[j,i]), ha=\"center\", va=\"center\", color=\"black\")\n",
    "            elif np.logical_and(blocks[0]==j,blocks[1]==i).any():\n",
    "                text = ax.text(i,j, 'X', ha=\"center\", va=\"center\", color=\"black\", backgroundcolor=\"black\")\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    im2 = ax2.imshow(Values, cmap=plt.get_cmap(\"Spectral\"))\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax2.set_xticks(np.arange(Lx))\n",
    "    ax2.set_yticks(np.arange(Ly))\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(Lx):\n",
    "        for j in range(Ly):\n",
    "            if np.logical_and(goal[0]==j, goal[1]==i).any():\n",
    "                text = ax2.text(i,j, 'G{}'.format(World[j,i]), ha=\"center\", va=\"center\", color=\"black\")\n",
    "            elif np.logical_and(blocks[0]==j,blocks[1]==i).any():\n",
    "                text = ax2.text(i,j, 'X', ha=\"center\", va=\"center\", color=\"black\", backgroundcolor=\"black\")\n",
    "            else:\n",
    "                text = ax2.text(i, j, '{:.2f}'.format(Values[j, i]), ha=\"center\", va=\"center\", color=\"black\")\n",
    "                \n",
    "                \n",
    "    plt.show()\n",
    "    # -------------------\n",
    "\n",
    "    \n",
    "\n",
    "def plot_world_values_policy(World, Values, Policy):\n",
    "    # ------------------\n",
    "    Ly, Lx = World.shape\n",
    "\n",
    "    fig, (ax, ax2, ax3) = plt.subplots(1,3)\n",
    "    im = ax.imshow(World, cmap=plt.get_cmap(\"Spectral\"))\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax.set_xticks(np.arange(Lx))\n",
    "    ax.set_yticks(np.arange(Ly))\n",
    "\n",
    "    goal = np.where(np.logical_or( World > 0.0, World < -1.0))\n",
    "    blocks = np.where(World == -1.0)\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(Lx):\n",
    "        for j in range(Ly):\n",
    "            if np.logical_and(goal[0]==j,goal[1]==i).any():\n",
    "                text = ax.text(i,j, 'G-{}'.format(World[j,i]), ha=\"center\", va=\"center\", color=\"black\")\n",
    "            elif np.logical_and(blocks[0]==j,blocks[1]==i).any():\n",
    "                text = ax.text(i,j, 'X', ha=\"center\", va=\"center\", color=\"black\", backgroundcolor=\"black\")\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    im2 = ax2.imshow(Values, cmap=plt.get_cmap(\"Spectral\"))\n",
    "\n",
    "    # We want to show all ticks...\n",
    "    ax2.set_xticks(np.arange(Lx))\n",
    "    ax2.set_yticks(np.arange(Ly))\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(Lx):\n",
    "        for j in range(Ly):\n",
    "            if np.logical_and(goal[0]==j, goal[1]==i).any():\n",
    "                text = ax2.text(i,j, 'G{}'.format(World[j,i]), ha=\"center\", va=\"center\", color=\"black\")\n",
    "                text = ax3.text(i,j, 'G{}'.format(World[j,i]), ha=\"center\", va=\"center\", color=\"black\")\n",
    "            elif np.logical_and(blocks[0]==j,blocks[1]==i).any():\n",
    "                text = ax2.text(i,j, 'X', ha=\"center\", va=\"center\", color=\"black\", backgroundcolor=\"black\")\n",
    "                text = ax3.text(i,j, 'X', ha=\"center\", va=\"center\", color=\"black\", backgroundcolor=\"black\")\n",
    "            else:\n",
    "                text = ax2.text(i, j, '{:.2f}'.format(Values[j, i]), ha=\"center\", va=\"center\", color=\"black\")\n",
    "    \n",
    "    im3 = ax3.imshow(Values, cmap=plt.get_cmap(\"Spectral\"))\n",
    "    X = np.arange(Lx)\n",
    "    Y = np.arange(Ly)\n",
    "    U, V = Policy[:,:,1], -Policy[:,:,0]\n",
    "    q = ax3.quiver(X, Y, U, V, color=\"black\")\n",
    "\n",
    "    plt.show()\n",
    "    # -------------------\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the world\n",
    "There will be no \"block\" cells as they are not needed for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "# TYPICAL (GRID)WORLD\n",
    "\n",
    "def new_world(Lx, Ly, goal, rewards):\n",
    "    \"\"\"\n",
    "    Construct a gridworld of width Lx and height Ly, \n",
    "    with a number of blocks Nblocks (to be distributed randomly)\n",
    "    and a list of tuple for positions of goal, and a list of corresponding rewards \n",
    "    \"\"\"\n",
    "    \n",
    "    # Checks that the number of goals is consistent with the number of rewards\n",
    "    assert len(goal) == len(rewards)\n",
    "    \n",
    "    # Constructs the empty matrix\n",
    "    World = np.zeros((Ly,Lx))\n",
    "    \n",
    "    # For all pairs of goals and rewards\n",
    "    for g, r in zip(goal, rewards):\n",
    "        World[g] = r\n",
    "    return World\n",
    "\n",
    "class World():\n",
    "    \"\"\"World environment class. It contains the grid, the datum and \n",
    "    goal positions, info on the current and 'explored grid' which contains\n",
    "    the last moment each block has been visited.\n",
    "    \n",
    "    NOTE: should add a variable that improves visibility of adjacent cells\n",
    "\n",
    "    Attributes:\n",
    "        grid:       grid of the world\n",
    "        goal:       position of target in grid\n",
    "        datum:      position of datum (reference) in grid\n",
    "        current:    current offset (future implementation)\n",
    "        explored:   contains time of last exploration of a cell\n",
    "        action:     List of action that the agent can take. I want these to be part of the environment\n",
    "        randomExplorerFlag:  if true, action is picked at random each time\n",
    "        rem_time:   remaining time to find the target\n",
    "        budget:     initial budget of time to find the target (will not be updated)\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, Ly = 20, Lx = 20, goal = (0,0), rem_time = 1000):\n",
    "        self.goal = goal\n",
    "        if self.goal == (0,0):\n",
    "            self.goal = (np.random.randint(Lx), np.random.randint(Ly))  #maybe this should be an array\n",
    "\n",
    "        self.grid = np.full((Lx, Ly), -1)\n",
    "        self.grid[goal] = 0\n",
    "        self.actions = np.array([[1,0],[-1,0],[0,1],[0,-1]])        #Actions = [Up, Down, Right, Left]\n",
    "        self.randomExplorerFlag = True\n",
    "        self.datum = (Lx//2, Ly//2)\n",
    "        self.budget = rem_time\n",
    "        self.rem_time = rem_time\n",
    "        self.current = (0,0)    #maybe for addition of current\n",
    "\n",
    "        self.explored = np.zeros_like(self.grid) \n",
    "\n",
    "    def plot_world(self, agent = None):\n",
    "        \"\"\"Plots the world (copied from tutor)\"\"\"\n",
    "        Ly, Lx = self.grid.shape\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        im = ax.imshow(self.grid, cmap=plt.get_cmap(\"Spectral\"))\n",
    "        \n",
    "        # We want to show all ticks...\n",
    "        ax.set_xticks(np.arange(Lx))\n",
    "        ax.set_yticks(np.arange(Ly))\n",
    "\n",
    "        # Loop over data dimensions and create text annotations.\n",
    "        for i in range(Lx):\n",
    "            for j in range(Ly):\n",
    "                if np.logical_and(self.goal[0]==j, self.goal[1]==i):\n",
    "                    text = ax.text(i,j, 'G'.format(int(self.grid[j,i])), ha=\"center\", va=\"center\", color=\"black\", backgroundcolor=\"blue\")\n",
    "                elif np.logical_and(self.datum[0]==j, self.datum[1]==i):\n",
    "                    text = ax.text(i,j, 'D', ha=\"center\", va=\"center\", color=\"black\", backgroundcolor=\"green\")\n",
    "                    pass\n",
    "                elif (agent != None) and (np.logical_and(agent.pos[0]==j, agent.pos[1]==i)):\n",
    "                    text = ax.text(i,j, 'A'.format(int(self.grid[j,i])), ha=\"center\", va=\"center\", color=\"black\", backgroundcolor=\"white\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_world_with_path(self, agent = None):\n",
    "\n",
    "        \"\"\"Plots the world (copied from tutor)\"\"\"\n",
    "        Ly, Lx = self.grid.shape\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        im = ax.imshow(self.grid, cmap=plt.get_cmap(\"Spectral\"))\n",
    "        \n",
    "        # We want to show all ticks...\n",
    "        ax.set_xticks(np.arange(Lx))\n",
    "        ax.set_yticks(np.arange(Ly))\n",
    "\n",
    "        # Loop over data dimensions and create text annotations.\n",
    "        for i in range(Lx):\n",
    "            for j in range(Ly):\n",
    "                if np.logical_and(self.goal[0]==j, self.goal[1]==i):\n",
    "                    text = ax.text(i,j, 'G'.format(int(self.grid[j,i])), ha=\"center\", va=\"center\", color=\"black\", backgroundcolor=\"blue\")\n",
    "                elif np.logical_and(self.datum[0]==j, self.datum[1]==i):\n",
    "                    text = ax.text(i,j, 'D', ha=\"center\", va=\"center\", color=\"black\", backgroundcolor=\"green\")\n",
    "\n",
    "        if (agent != None):\n",
    "            for cell in agent.explored:\n",
    "                text = ax.text(cell[1], cell[0], '.'.format(int(self.grid[cell[1],cell[0]])), ha=\"center\", va=\"center\", color=\"white\", backgroundcolor=\"white\")\n",
    "        \n",
    "        text = ax.text(i,j, 'A'.format(int(self.grid[agent.pos[1],agent.pos[0]])), ha=\"center\", va=\"center\", color=\"black\", backgroundcolor=\"white\")\n",
    "    \n",
    "        plt.show()\n",
    "\n",
    "    def plot_world_policy(self, actions):\n",
    "        \n",
    "        policy = [[1, 0] if x == 0 else\n",
    "                 [-1, 0] if x == 1 else\n",
    "                 [0, 1] if x == 2 else\n",
    "                 [0, -1] for x in actions]\n",
    "\n",
    "        Ly, Lx = self.grid.shape\n",
    "        fig, ax = plt.subplots()\n",
    "        im = ax.imshow(self.grid, cmap=plt.get_cmap(\"Spectral\"))\n",
    "        X = np.arange(Lx)\n",
    "        Y = np.arange(Ly)\n",
    "        U, V = policy[:,:,1], -policy[:,:,0]\n",
    "        q = ax3.quiver(X, Y, U, V, color=\"black\")\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Agent = explorator\"\"\"\n",
    "    def __init__(self, world):\n",
    "        self.pos = (world.datum)    #initial position is the same as the datum\n",
    "        self.visibility = 1     #try to change visibility of agent (sees not only strictly adjacent cells)\n",
    "\n",
    "        self.env = world        #environment of agent. I hope this isn't a full copy of the world\n",
    "        self.ddist = 0          #distance from datum\n",
    "        self.choices = np.zeros_like(world.grid)    #contains last choices for each position\n",
    "        self.explored = [self.pos]      #array with already explored cells\n",
    "\n",
    "    \n",
    "    def chooseAction(self):\n",
    "        if self.env.randomExplorerFlag:\n",
    "            # Get the shape of the array\n",
    "            num_rows, num_cols = self.env.actions.shape\n",
    "\n",
    "            # Generate random indices for row and column\n",
    "            random_row = np.random.randint(num_rows)\n",
    "\n",
    "            # Pick the random element using the random indices\n",
    "            action = self.env.actions[random_row]\n",
    "\n",
    "        else:\n",
    "            print(\"Needs implementation for randomExplorerFlag = False\")\n",
    "        return action\n",
    "    \n",
    "    def transition(self, A):\n",
    "        \"\"\"Takes the selected action A and returns the resulting new S.\n",
    "        Updates remaining time in environment attribute of the agent\"\"\"\n",
    "        # I try to move\n",
    "        S_new = self.pos + A\n",
    "        Ly, Lx = self.env.grid.shape\n",
    "        # if I go out of the world, I stay still\n",
    "        if ((S_new[0] == Ly) or np.any(S_new == -1) or (S_new[1] == Lx)):\n",
    "            #print(\"Invalid action proposal, staying here.\")\n",
    "            S_new = self.pos\n",
    "        else:\n",
    "            #update choices matrix\n",
    "            self.choices[self.pos] = np.where((self.env.actions == A).all(axis=1))[0][0]   #stores index of action (one number per cell of choices matrix)\n",
    "\n",
    "        # get the reward for the new state\n",
    "        reward = self.env.grid[S_new[0], S_new[1]]\n",
    "        # reward is also influenced by the distance\n",
    "\n",
    "\n",
    "        # add cell to list of visited ones\n",
    "        self.explored = np.append(self.explored, [S_new], axis=0)\n",
    "\n",
    "        #update last visit of this cell in environment attribute\n",
    "        self.env.explored[self.pos] = self.env.budget - self.env.rem_time\n",
    "\n",
    "        #update remaining time in environment attribute\n",
    "        self.env.rem_time = self.env.rem_time + reward\n",
    "\n",
    "        # returns the new state\n",
    "        return S_new\n",
    "\n",
    "    def search(self):\n",
    "        i = 0\n",
    "        while np.any(self.pos != self.env.goal) and (i in range(self.env.rem_time)):\n",
    "            #choose action\n",
    "            action = self.chooseAction()\n",
    "            #perform action\n",
    "            self.pos  = self.transition(action)\n",
    "            #update info relative to datum\n",
    "            self.ddist = math.dist(self.pos, self.env.datum)\n",
    "\n",
    "        # if np.all(self.pos == self.env.goal):\n",
    "        #     print(f\"Target found with {self.env.budget - self.env.rem_time} iterations\")\n",
    "        #     print(\"Weight would be \", self.env.rem_time / self.env.budget)\n",
    "        # else:\n",
    "        #     print(\"Target not found\")\n",
    "\n",
    "        #end run and return remaining time\n",
    "        return self.env.rem_time/self.env.budget\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lx = 20\n",
    "Ly = 20\n",
    "n_runs = 1000\n",
    "\n",
    "weights = np.zeros(n_runs)\n",
    "action_mtx = np.zeros((Lx, Ly))\n",
    "\n",
    "for i in range(n_runs):\n",
    "    goal = (np.random.randint(Lx), np.random.randint(Ly))\n",
    "    world = World(Ly, Lx, goal)\n",
    "    ag = Agent(world)\n",
    "    weights[i] = ag.search()\n",
    "\n",
    "    for row in range(Lx):\n",
    "        for col in range(Ly):\n",
    "            action_mtx[row, col] = action_mtx[row, col] + ag.choices[row, col]*weights[i]\n",
    "            #action_mtx[row, col] = action_mtx[row, col] + ag.choices[row, col]\n",
    "\n",
    "\n",
    "action_mtx = action_mtx/(np.sum(weights))\n",
    "#action_mtx = action_mtx/(n_runs)\n",
    "\n",
    "print(\"ag.choices = \", ag.choices)\n",
    "\n",
    "print(\"weights = \", weights)\n",
    "print(\"action matrix: \")\n",
    "print(action_mtx)\n",
    "#world.plot_world_policy(ag.choices)\n",
    "\n",
    "print(\"Visual representation of the gridworld:\")\n",
    "world.plot_world_with_path(ag)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transition and Rewards\n",
    "\n",
    " - Transitions: Given a state S and the Action A, we return the new state, taking care that we do not do forbidden actions. We will use deterministic transitions.\n",
    "\n",
    " - Rewards: Given a state S and the Action A and the new state S', we also have the probability to receive a reward R.\n",
    "\n",
    "__PS: Achtung!__\n",
    "The convention for python arrays is $A[i_y, i_x]$, where $i_y$ is the row-index and $i_x$ is the column-index... So the convention with _up_, _down_, _right_ and _left_ directions consistent, but may be a bit confusing: Use special care!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of actions I can take: Actions = [Up, Down, Right, Left]\n",
    "Actions = np.array([[1,0],[-1,0],[0,1],[0,-1]])\n",
    "\n",
    "def transition(S, A, world):\n",
    "    \"\"\"\n",
    "    Takes the current position S and selected action A,\n",
    "    and returns the resulting new S given a world World.\n",
    "    \"\"\"\n",
    "    # I try to move\n",
    "    S_new = S + A\n",
    "    Ly, Lx = world.grid.shape\n",
    "    # if I go out of the world, I stay still\n",
    "    if ((S_new[0] == Ly) or np.any(S_new == -1) or (S_new[1] == Lx)):\n",
    "        S_new = S\n",
    "    # if I found a block I stay still\n",
    "    elif world.grid[S_new[0],S_new[1]] == -1:\n",
    "        S_new = S\n",
    "    # returns the new state\n",
    "    return S_new \n",
    "\n",
    "def rewards(S, A, S_new, world):\n",
    "    \"\"\"\n",
    "    Takes the current position S and selected action A,\n",
    "    and returns the resulting reward given that it has ended up in S_new and \n",
    "    the gridworld is World.\n",
    "    \"\"\"\n",
    "    # reward is always zero...\n",
    "    reward = 0\n",
    "    # expect when I reach the final goal\n",
    "    if (world.grid[S_new[0], S_new[1]] > 0) or (world.grid[S_new[0], S_new[1]] < -1):\n",
    "        reward = world.grid[S_new[0], S_new[1]]\n",
    "    # return the reward\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_values(Values, world, gamma, p=0.9):\n",
    "    \"\"\"\n",
    "    Takes the current matrix of values (V_k(s) )\n",
    "    The associated gridworld,\n",
    "    And computes the bellman operator   \n",
    "    V_(k+1) (s) = max_a { sum_s'r   p(r, s'| s, a)(r + gamma V_k(s') }\n",
    "    And the relative best policy\n",
    "    pi_(k+1)(s) = argmax_a { sum_s'r   p(r, s'| s, a)(r + gamma V_k(s') }\n",
    "    \n",
    "    In the case with deterministic (p=1) or stochastic (p<1) actions.\n",
    "    Returns V_(k+1)(s) and pi_(k+1)(s)\n",
    "    \"\"\"\n",
    "    \n",
    "    # -----------------------------------------------------------\n",
    "    # The dimension of the world\n",
    "    Ly, Lx = world.shape\n",
    "    # initialize the vectors to store the new values and policy\n",
    "    NewValues = np.zeros((Ly,Lx))\n",
    "    NewPolicy = np.zeros((Ly,Lx,2))\n",
    "    # \n",
    "    goal = np.where(np.logical_or( world.grid > 0.0, world.grid < -1.0))\n",
    "    \n",
    "    # --------------- UPDATE -------------------------------------\n",
    "    # cycle over all the states\n",
    "    for ix in range(Lx):\n",
    "        for iy in range(Ly):\n",
    "            # state is defined by its indices\n",
    "            S = np.array([iy, ix])\n",
    "            \n",
    "            # skip blocked squares\n",
    "            if world.grid[S[0],S[1]] != -1:\n",
    "                maxvalue = -100\n",
    "                \n",
    "                # it \"tries out\" all actions and store the best\n",
    "                for A in Actions:\n",
    "                    new_S = transition(S, A, world.grid)\n",
    "                    R = rewards(S, A, new_S, world.grid)\n",
    "\n",
    "                    value_action = 0.0\n",
    "                    \n",
    "                    value_action += R + gamma*Values[new_S[0], new_S[1]]\n",
    "                    \n",
    "                    if (value_action > maxvalue):\n",
    "                        maxvalue = value_action\n",
    "                        bestact = A\n",
    "                    # -------------------------\n",
    "                    # Q: What happens in a tie?\n",
    "                    # -------------------------\n",
    "\n",
    "                # It stores the new value and policy of state S\n",
    "                NewValues[S[0],S[1]] = maxvalue\n",
    "                NewPolicy[S[0],S[1],:] = bestact\n",
    "    # --------------------------------------------------------------\n",
    "\n",
    "    # --------------------------------------------------------------\n",
    "    for gx, gy in zip(goal[0],goal[1]):\n",
    "        NewValues[gx, gy] = 0\n",
    "        NewPolicy[gx, gy] = [0,0]\n",
    "    # --------------------------------------------------------------\n",
    "    return NewValues, NewPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of [0, -1] : [3]\n",
      "Type of target element: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Define the array\n",
    "arr = np.array([[1, 0], [-1, 0], [0, 1], [0, -1]])\n",
    "\n",
    "# Define the element you want to find the index of\n",
    "target_element = [0, -1]\n",
    "\n",
    "# Use numpy.where() to find the index of the target_element\n",
    "indices = np.where((arr == target_element).all(axis=1))\n",
    "\n",
    "# If the element is found, the indices variable will contain the index/indices of the target_element in the array\n",
    "if len(indices[0]) > 0:\n",
    "    print(\"Index of\", target_element, \":\", indices[0])\n",
    "    print(\"Type of target element:\", type(target_element))\n",
    "else:\n",
    "    print(\"Element not found in the array.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
