{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Piero PettenÃ  - RL project  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the distance class of a given distance, given the size of the grid L\n",
    "# distance 0 -> distance less than L/10\n",
    "# distance 1 -> distance between L/10 and L/5\n",
    "# distance 2 -> distance between L/5 and L\n",
    "def get_dist_class(dist: float, L: int):\n",
    "    if dist < L / 10:\n",
    "        dist = 0\n",
    "    elif dist < L / 5:\n",
    "        dist = 1\n",
    "    elif dist <= L:\n",
    "        dist = 2\n",
    "    else:\n",
    "        print(f\"Error: distance out of bounds: dist={dist}\")\n",
    "        return -1\n",
    "    return dist\n",
    "\n",
    "\n",
    "# Gets the time class of a given time, given the budget of time B\n",
    "# time 0 -> time less than B/4\n",
    "# time 1 -> time between B/4 and B/2\n",
    "# time 2 -> time between B/2 and 3*B/4\n",
    "# time 3 -> time between 3*B/4 and B\n",
    "def get_time_class(time: int, B: int):\n",
    "    if time < B / 4:\n",
    "        time = 0\n",
    "    elif time < B / 2:\n",
    "        time = 1\n",
    "    elif time < 3 * B / 4:\n",
    "        time = 2\n",
    "    elif time <= B:\n",
    "        time = 3\n",
    "    else:\n",
    "        print(f\"Error: time out of bounds: time={time}\")\n",
    "        return -1\n",
    "    return time\n",
    "\n",
    "# Function that discretizes the state. It works with SQUARE GRIDS\n",
    "def discretize_state(grid_size: int, budget: int, ddist: float, rem_time: int):\n",
    "    dist = get_dist_class(dist=ddist, L=grid_size)\n",
    "    time = get_time_class(time=rem_time, B=budget)\n",
    "    return dist, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of a state\n",
    "1. Distance of agent from datum\n",
    "2. Direction of search movement\n",
    "3. Remaining time to find target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the q function\n",
    "1. Distance of agent from datum\n",
    "2. Direction of search movement\n",
    "3. Remaining time to find target\n",
    "4. Possible action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TYPICAL (GRID)WORLD\n",
    "\n",
    "\n",
    "class World:\n",
    "    \"\"\"World environment class. It contains the grid, the datum and\n",
    "    goal positions, info on the current and 'explored grid' which contains\n",
    "    the last moment each block has been visited.\n",
    "\n",
    "    NOTE: should add a variable that improves visibility of adjacent cells\n",
    "\n",
    "    Attributes:\n",
    "        grid:                   grid of the world. cell contains -1 if not explored, -2 if explored, 10 if goal\n",
    "        goal:                   position of target in grid\n",
    "        datum:                  position of datum (reference) in grid\n",
    "        current:                current offset (future implementation)\n",
    "        actions:                List of action that the agent can take. I want these to be part of the environment\n",
    "        randomExplorerFlag:     if true, action is picked at random each time\n",
    "        rem_time:               remaining time to find the target\n",
    "        budget:                 initial budget of time to find the target (will not be updated)\n",
    "        states:                 states q values of the table\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, Ly: int=20, Lx: int =20, goal: np.ndarray =np.array([0, 0]), rem_time: int =1000):\n",
    "        self.goal = goal\n",
    "        if np.array_equal(self.goal, [0,0]):\n",
    "            self.goal = np.array([np.random.randint(Lx), np.random.randint(Ly)])\n",
    "\n",
    "        self.grid = np.full(\n",
    "            shape=(Lx, Ly), fill_value= -1\n",
    "        )  # Every cell which has not been explored contains -1\n",
    "\n",
    "        self.grid[goal[0], goal[1]] = 10  # Goal cell contains 10\n",
    "        self.actions = np.array(\n",
    "            [[1, 0], [-1, 0], [0, 1], [0, -1]]\n",
    "        )  # Actions = [Up, Down, Right, Left]\n",
    "        self.randomExplorerFlag = True\n",
    "        self.datum = np.array([Lx // 2, Ly // 2])\n",
    "        self.budget = rem_time\n",
    "        self.rem_time = rem_time\n",
    "        self.current = np.array([0, 0])  # maybe for addition of current\n",
    "        # state is now defined as a 3d numpy array of (distance, dir, time)\n",
    "        self.states = np.zeros(\n",
    "            shape=(3, len(self.actions), 4)\n",
    "        )  # ***************************************************************************\n",
    "        # Convert the \"shape()\" parameter to a variable that we can choose\n",
    "        # i.e. try to generalize this code for different number of distance classes,\n",
    "        # directions and time classes\n",
    "\n",
    "\n",
    "    # Function that simply plots the grid\n",
    "    def plot_world(self):\n",
    "        cmap = plt.get_cmap(\"viridis\")  # You can choose a different colormap if you prefer\n",
    "\n",
    "        # Define the colors for each value in the array\n",
    "        colors = {-2: \"gray\", -1: \"white\", 10: \"red\"}\n",
    "\n",
    "        # Create a figure and axis\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        # Plot each cell with the corresponding color\n",
    "        for i in range(self.grid.shape[0]):\n",
    "            for j in range(self.grid.shape[1]):\n",
    "                value = self.grid[i, j]\n",
    "                color = colors.get(\n",
    "                    value, \"white\"\n",
    "                )  # Default to white if the value is not in the colors dictionary\n",
    "                ax.add_patch(\n",
    "                    plt.Rectangle((j, i), 1, 1, fill=True, color=color, edgecolor=\"black\")\n",
    "                )\n",
    "\n",
    "                # Display the cell value in the center\n",
    "                ax.text(\n",
    "                    j + 0.5,\n",
    "                    i + 0.5,\n",
    "                    str(value),\n",
    "                    color=\"black\",\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    fontsize=10,\n",
    "                )\n",
    "\n",
    "        # Set axis limits\n",
    "        ax.set_xlim(0, self.grid.shape[1])\n",
    "        ax.set_ylim(0, self.grid.shape[0])\n",
    "\n",
    "        # Set major ticks to be at the center of cells\n",
    "        ax.set_xticks(np.arange(0, self.grid.shape[1] + 1, 1))\n",
    "        ax.set_yticks(np.arange(0, self.grid.shape[0] + 1, 1))\n",
    "\n",
    "        # Remove minor ticks\n",
    "        ax.set_xticks([], minor=True)\n",
    "        ax.set_yticks([], minor=True)\n",
    "\n",
    "        # Set grid lines to be at major ticks\n",
    "        ax.grid(which=\"major\", color=\"black\", linewidth=2)\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Agent = explorator\n",
    "\n",
    "    Attributes:\n",
    "        pos:            current position of agent\n",
    "        visibility:     how far the agent can see (1 means only adjacent cells)\n",
    "        env:\n",
    "        action_value:   State-Action value matrix\n",
    "        ddist:          distance from datum\n",
    "        ??choices:        last choices for each position\n",
    "        ##explored:       matrix with already explored cells -> already in environment\n",
    "        dir:            direction of movement (=last action)\n",
    "        state:          state q values of the table\n",
    "        q function:     q(state, action)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, world: World):\n",
    "        # Defining some constants\n",
    "        self.n_actions = len(world.actions)\n",
    "        self.Ly, self.Lx = world.grid.shape\n",
    "\n",
    "        self.pos = world.datum  # initial position is the same as the datum\n",
    "        self.visibility = 1  # try to change visibility of agent (sees not only strictly adjacent cells)\n",
    "        self.env = world  # environment of agent. Should be a reference, not a full copy\n",
    "        # self.action_value = np.zeros((world.grid.shape, len(world.actions)))\n",
    "        self.ddist = 0  # distance from datum\n",
    "        # self.explored = np.zeros_like(world.grid)  # matrix with already explored cells\n",
    "        self.dir = np.array([0, 0])\n",
    "        self.state_space = np.array(\n",
    "            list(itertools.product(range(3), range(4), range(4)))\n",
    "        )  # where 3 is the number of distance classes and 4 the number of directions, 4 the number of time classes\n",
    "        self.actions = world.actions\n",
    "        self.q_table = np.zeros((len(self.state_space), self.n_actions))\n",
    "\n",
    "\n",
    "    # Returns the 4 nearby cells to visit. This could be extended to diagonal movement.\n",
    "    def get_nearby_cells(self):\n",
    "        top = self.pos + [1, 0]\n",
    "        bottom = self.pos + [-1, 0]\n",
    "        right = self.pos + [0, 1]\n",
    "        left = self.pos + [0, -1]\n",
    "        nearby = np.array([bottom, top, left, right])\n",
    "\n",
    "        return nearby\n",
    "\n",
    "    def remove_outside_cells(self, coordinates: np.ndarray):\n",
    "        \"\"\"\n",
    "        Remove coordinates outside of a grid of size Lx by Ly.\n",
    "\n",
    "        Parameters:\n",
    "        - coordinates: numpy array of shape (n, 2) representing 2D integer coordinates.\n",
    "        - Lx: int, size of the grid along the x-axis.\n",
    "        - Ly: int, size of the grid along the y-axis.\n",
    "\n",
    "        Returns:\n",
    "        - filtered_coordinates: numpy array with valid coordinates within the grid.\n",
    "        \"\"\"\n",
    "        # Check if the input is a numpy array\n",
    "        if not isinstance(coordinates, np.ndarray):\n",
    "            raise ValueError(\"Input 'coordinates' must be a NumPy array.\")\n",
    "\n",
    "        # Check if the shape of the array is (n, 2)\n",
    "        if len(coordinates.shape) != 2 or coordinates.shape[1] != 2:\n",
    "            raise ValueError(\n",
    "                \"Input 'coordinates' must be a 2D array with shape (n, 2).\"\n",
    "            )\n",
    "\n",
    "        # Create a boolean mask for valid coordinates\n",
    "        valid_mask = (\n",
    "            (0 <= coordinates[:, 0])\n",
    "            & (coordinates[:, 0] < self.Lx)\n",
    "            & (0 <= coordinates[:, 1])\n",
    "            & (coordinates[:, 1] < self.Ly)\n",
    "        )\n",
    "\n",
    "        # Use boolean indexing to filter the coordinates array\n",
    "        filtered_coordinates = coordinates[valid_mask]\n",
    "\n",
    "        return filtered_coordinates\n",
    "\n",
    "    # Function that chooses an action for the agent. It can be random or based on the action-value matrix.\n",
    "    # Validity of the action is checked before it is returned.\n",
    "    # It has to be inserted in a while loop which doesn't update self.env.rem_time, otherwise remaining time\n",
    "    # will decrease even when an action is not taken but found to be invalid.\n",
    "    # This is bad especially if the agent can see more than one cell at a time, as it is increasingly\n",
    "    # more likely that it runs into already visited cells (which are considered not valid actions).\n",
    "    def chooseAction(self):\n",
    "        if self.env.randomExplorerFlag:\n",
    "            # border check:\n",
    "            nearby = self.get_nearby_cells()\n",
    "            nearby = self.remove_outside_cells(nearby)\n",
    "\n",
    "            while True:\n",
    "                # Sample from self.actions\n",
    "                action = random.choice(list(self.actions))\n",
    "                S_new = self.pos + np.array(action)\n",
    "\n",
    "                # Check if the resulting position is inside nearby array\n",
    "                if any(np.array_equal(S_new, cell) for cell in nearby):\n",
    "                    break\n",
    "\n",
    "            return action\n",
    "\n",
    "        else:\n",
    "            print(\"Needs implementation for randomExplorerFlag = False. Returning -5\")\n",
    "        return -5  # clear error\n",
    "\n",
    "    # Function that returns the reward of the system. If the agent is in the goal position, it returns 10.\n",
    "    # It has to be called whenever an action is taken, as it only checks the current position of the agent.\n",
    "    # Here we could implement the possibility of viewing nearby cells.\n",
    "    def reward(self):\n",
    "        # idea: reward is also influenced by the distance\n",
    "        # reward = reward - self.ddist\n",
    "        return self.env.grid[self.pos[0], self.pos[1]]\n",
    "\n",
    "    # Function that chooses an action using \"chooseAction\" and updates the position of the agent.\n",
    "    # It also updates the direction of the agent and the visited cells.\n",
    "    # It only updates the position if the action is not [0,0].\n",
    "    def update_position(self):\n",
    "        action = np.array([0, 0])\n",
    "        while np.array_equal(action, [0, 0]):\n",
    "            action = self.chooseAction()\n",
    "\n",
    "        self.pos = self.pos + action\n",
    "        self.dir = action  # update direction\n",
    "        self.env.grid[self.pos[0], self.pos[1]] = -2  # update visited cells\n",
    "\n",
    "        return action\n",
    "\n",
    "    # Function that updates the state of the agent. It has to be called after the position has been updated.\n",
    "    # Note, the dir returned value is the index of the chosen action from the vector  \n",
    "    # actions = np.array([[1, 0], [-1, 0], [0, 1], [0, -1]])\n",
    "    def get_current_state(self):\n",
    "        \"\"\"Return current state of the agent.\"\"\"\n",
    "        ddist = np.linalg.norm(self.pos - self.env.datum)\n",
    "        dir = np.where((self.actions == self.dir).all(axis=1)) # get index of chosen action\n",
    "        # Discretize distance and time:\n",
    "        dist, time = discretize_state(\n",
    "            grid_size=self.Lx,  # supposing square grid\n",
    "            budget=self.env.budget,\n",
    "            ddist=ddist,\n",
    "            rem_time=self.env.rem_time,\n",
    "        )\n",
    "\n",
    "        return np.array([dist, dir[0][0], time])\n",
    "\n",
    "    def update_q_table(self, reward, action_idx):\n",
    "        \"\"\"Updates the state of the agent\"\"\"\n",
    "        # parameters that need to be decided:\n",
    "        alpha = 0.1  # Learning rate\n",
    "        gamma = 0.9  # Discount factor\n",
    "        reward = 1.0  # Immediate reward\n",
    "\n",
    "        state= self.get_current_state()\n",
    "        state_index = np.where(np.all(self.state_space == state, axis=1))[0][0]\n",
    "        self.q_table[state_index, action_idx] = (1 - alpha) * self.q_table[state_index, action_idx] +\\\n",
    "            alpha * (reward + gamma * np.max(self.q_table[state_index, :]))\n",
    "\n",
    "    def transition(self):\n",
    "        \"\"\"Chooses a valid action, updates the position of the agent and the remaining time.\"\"\"\n",
    "        action = self.update_position()\n",
    "        # print(f\"Chosen action = {action}\")\n",
    "        action_idx = np.where((self.actions == action).all(axis=1))[0][0]\n",
    "        reward = self.reward()\n",
    "        # print(f\"Reward  = {reward}\")\n",
    "        self.update_q_table(reward, action_idx)\n",
    "\n",
    "        # update remaining time in environment attribute\n",
    "        self.env.rem_time -= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lx = 20\n",
    "Ly = 20\n",
    "\n",
    "world = World(Lx=Lx, Ly=Ly, rem_time=100, goal=np.array([5, 5]))\n",
    "ag = Agent(world)\n",
    "actions = {}\n",
    "\n",
    "while(world.rem_time > 0):\n",
    "    ag.transition()\n",
    "\n",
    "print(f\"Q table: \\n{ag.q_table}\")\n",
    "world.plot_world()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
