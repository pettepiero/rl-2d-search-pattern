{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Piero Petten√† -started from Zhang's version of normal grid world found here:\n",
    "# https://github.com/MJeremy2017/reinforcement-learning-implementation/blob/master/GridWorld/gridWorld.py#L11C1-L18C39\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "N_ROWS = 40\n",
    "N_COLS = 40\n",
    "IN_TARGET_POS = (3,3)\n",
    "START = (N_ROWS//2, N_COLS//2)\n",
    "TARGET_POS = IN_TARGET_POS\n",
    "DATUM_POS = [4,4]\n",
    "IN_BUDGET = 1000\n",
    "\n",
    "# you need to implement ways of updating DATUM_POS and TARGET_POS wrt a flow of current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Up \t Down \t Left \t Right\n",
      "245 \t 254 \t 261 \t 240\n",
      "<class 'tuple'> <class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "class State:\n",
    "    def __init__(self, state=START):\n",
    "        self.board = np.zeros([N_ROWS, N_COLS])\n",
    "        self.board[*TARGET_POS] = 1\n",
    "        self.isEnd = False\n",
    "        self.position = state          #starting position when initializing state\n",
    "        self.datum_pos = DATUM_POS\n",
    "        self.datum_dist = 0\n",
    "        self.distance = 0 \n",
    "        \n",
    "    def isEndFunc(self):\n",
    "        if self.position == TARGET_POS:\n",
    "            self.isEnd = True\n",
    "\n",
    "    # returns x and y distance between current position and datum\n",
    "    # position of datum is always known for simplicity\n",
    "    def relativePos(self):\n",
    "        dx = self.datum_pos[0]-self.position[0]\n",
    "        dy = self.datum_pos[1]-self.position[1]\n",
    "        dpos = np.sqrt(dx**2 + dy**2)\n",
    "\n",
    "        return dx, dy, dpos\n",
    "\n",
    "\n",
    "    #returns next position given an action\n",
    "    def nxtPosition(self, action):\n",
    "        \"\"\"\n",
    "        action: up, down, left, right\n",
    "        -------------\n",
    "        0 | 1 | 2| 3|\n",
    "        1 |\n",
    "        2 |\n",
    "        return next position\n",
    "        \"\"\"\n",
    "        \n",
    "        if action == \"up\":\n",
    "            nxtState = (self.position[0] - 1, self.position[1])\n",
    "        elif action == \"down\":\n",
    "            nxtState = (self.position[0] + 1, self.position[1])\n",
    "        elif action == \"left\":\n",
    "            nxtState = (self.position[0], self.position[1] - 1)\n",
    "        else:\n",
    "            nxtState = (self.position[0], self.position[1] + 1)\n",
    "        # if next state legal\n",
    "        if 0 <= nxtState[0] <= N_ROWS -1:       #this might give rise to errors\n",
    "            if 0 <= nxtState[1] <= N_COLS -1:\n",
    "                    return nxtState\n",
    "        return self.position   # illegal new state, remain here\n",
    "\n",
    "    def showBoard(self):\n",
    "        self.board[self.position] = 1\n",
    "        for i in range(0, N_ROWS):\n",
    "            print('-----------------')\n",
    "            out = '| '\n",
    "            for j in range(0, N_COLS):\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = '0'\n",
    "                else:\n",
    "                    token = '*'\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-----------------')\n",
    "\n",
    "\n",
    "# Agent of player\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        self.state = State()\n",
    "        self.lr = 0.2\n",
    "        self.exp_rate = 0.3\n",
    "        self.rewards = IN_BUDGET    # the idea is to remove 1 from initial rewards until reaching 0,\n",
    "                                    # this is to simulate a \"time out\"\n",
    "        # initial state reward\n",
    "        # we are mapping states to values. Then we will pick the state with max value\n",
    "        self.state_values = {}\n",
    "        for i in range(N_ROWS):\n",
    "            for j in range(N_COLS):\n",
    "                self.state_values[(i, j)] = 0  # set initial value to 0\n",
    "        self.stats = {'up': 0, 'down': 0, 'left': 0, 'right': 0}\n",
    "\n",
    "    def chooseAction(self):\n",
    "        # choose action with most expected value \n",
    "        ##############################################\n",
    "        # NOTE: this might be giving the action 'right' many more opportunities because it is the last one in the actions vector\n",
    "\n",
    "        mx_nxt_reward = 0\n",
    "        action = \"\"\n",
    "\n",
    "        # if np.random.uniform(0, 1) <= self.exp_rate:        #with probability rl choose to pick random action (uniformly)\n",
    "            # action = np.random.choice(self.actions)\n",
    "        # else:                                               #otherwise pick the action that corresponds to cell with max value\n",
    "            # # greedy action\n",
    "            # for a in self.actions:\n",
    "                # # if the action is deterministic\n",
    "                # nxt_reward = self.state_values[self.state.nxtPosition(a)]\n",
    "                # if nxt_reward >= mx_nxt_reward:\n",
    "                    # action = a\n",
    "                    # mx_nxt_reward = nxt_reward\n",
    "        action = np.random.choice(self.actions)\n",
    "        return action\n",
    "\n",
    "    def updateRewards(self):\n",
    "        if self.state.position != TARGET_POS:\n",
    "            self.rewards = self.rewards -1\n",
    "        else:\n",
    "            self.state.isEnd = True\n",
    "            \n",
    "    #update state and appends position to history of agent\n",
    "    def takeAction(self, action):\n",
    "        position = self.state.nxtPosition(action)\n",
    "        self.state.position = position\n",
    "\n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        self.state = State()\n",
    "\n",
    "    def play(self):\n",
    "        while self.rewards > 0 and not(self.state.isEnd):\n",
    "            # to the end of game back propagate reward\n",
    "            action = self.chooseAction()\n",
    "            # append trace\n",
    "            self.states.append(self.state.nxtPosition(action))          #maybe first position will be missing\n",
    "            #print(\"current position {} action {}\".format(self.state.position, action))\n",
    "            # by taking the action, it reaches the next state\n",
    "            self.takeAction(action)\n",
    "            self.updateRewards()\n",
    "            self.updateStats(action)\n",
    "            # mark is end\n",
    "            self.state.isEndFunc()\n",
    "            #print(\"nxt state\", self.state.position)\n",
    "            #print(\"---------------------\")\n",
    "\n",
    "        if self.state.isEnd:\n",
    "            # back propagate\n",
    "            #reward = self.rewards\n",
    "            # explicitly assign end state to reward values\n",
    "            #self.state_values[self.state.position] = reward  # this is optional\n",
    "            #print(\"Game End Reward\", reward)\n",
    "            print(\"Target found\")\n",
    "            #for s in reversed(self.states):\n",
    "            #    reward = self.state_values[s] + self.lr * (reward - self.state_values[s])\n",
    "            #     self.state_values[s] = round(reward, 3)\n",
    "            # self.reset()\n",
    "\n",
    "    def updateStats(self, action):\n",
    "        self.stats[action] = self.stats[action] +1\n",
    "\n",
    "    def showStats(self):\n",
    "        print(\"Up \\t Down \\t Left \\t Right\")\n",
    "        print(f\"{self.stats['up']} \\t {self.stats['down']} \\t {self.stats['left']} \\t {self.stats['right']}\")\n",
    "\n",
    "    def showValues(self):\n",
    "        for i in range(0, N_ROWS):\n",
    "            print('----------------------------------')\n",
    "            out = '| '\n",
    "            for j in range(0, N_COLS):\n",
    "                out += str(self.state_values[(i, j)]).ljust(6) + ' | '\n",
    "            print(out)\n",
    "        print('----------------------------------')\n",
    "\n",
    "    # saves path to png file\n",
    "    def exportFig(self):\n",
    "        path_fig = Image.new('RGB', (N_ROWS, N_COLS), 'black')\n",
    "        path_fig.putpixel((TARGET_POS), (255, 0, 0))\n",
    "\n",
    "        for x,y in self.states:\n",
    "            path_fig.putpixel((x,y), (255,255,255))\n",
    "\n",
    "        for x in range(N_ROWS):\n",
    "            path_fig.putpixel((x, 0), (255, 83, 73))\n",
    "            path_fig.putpixel((x, N_ROWS-1), (255, 83, 73))\n",
    "\n",
    "        for y in range(N_COLS):\n",
    "            path_fig.putpixel((0, y), (255, 83, 73))\n",
    "            path_fig.putpixel((N_COLS-1, y), (255, 83, 73))\n",
    "\n",
    "        path_fig.save('path_fig.png')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ag = Agent()\n",
    "    ag.play()\n",
    "    ag.exportFig()\n",
    "    ag.showStats()\n",
    "    print(type(ag.state.position), type(TARGET_POS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
